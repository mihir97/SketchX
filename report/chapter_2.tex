%_____________________________________________________________________________________________
% Department of Comp/IT BTech Project Report
%_____________________________________________________________________________________________

\chapter{Literature Review}

  \section{Pix2code}
    Transforming a graphical user interface screenshot created by a designer into computer code is a typical task conducted by a developer in order to build customized software, websites, and mobile applications. In this paper, deep learning methods are  leveraged to train a model end-to-end to automatically generate code from a single input image with over 77% of accuracy for three different platforms (i.e. iOS, Android and web-based technologies).
    The project focuses only on the front end aspect and has little or no consideration of the backend integration required. It aims mobile and web platform underlining the fact that transforming images to component specification files is the important step, the later part of code generation can be made flexible and pluggable. This project was one of the first papers which discussed the approach of using deep learning techniques to generate interfaces based on wireframes.

  \section{Sketch2Code}
    This is a experimental project developed by a team at Microsoft AI Lab the project demonstrates the feasibility of the idea. It also provides datasets for training Neural networks. This project accepts hand drawn mockups as input and generates simple HTML code. The accuracy the project provides is comparatively less, although the image tagging and component file generation is strong.
    Currently this project is published and hosted on azure. Although the results generated are not very accurate and the HTML based alignment of components is inaccurate. Another peculiarity about this project is that it uses neural network as a service based on Microsoft custom vision API. This is a innovative approach to move complex neural networks to the cloud and simplify the process of training and testing.

  \section{Airbnb design team}
    Airbnb design team is working on a project which would convert hand drawn wireframes to React applications. This project is based on a end to end approach and uses Neural networks to both identify elements in the sketch and to convert the structured data of element locations into code. Their aim is to reduce the the development time for airbnb products. The company states this is a experimental project and an ongoing exploration.No source code has been provided as the project is in very early stage. The project shows promising progress a  demo showcase itâ€™s engine working in real time meaning the model is lightweight and proves the potential this idea has. Although the documentation states that the training set is limited and needs improvement for improved results on varied data sets

  \section{Deepcoder}
    This is a very noteworthy paper published in 2017, the paper discusses the possibility of computers writing code themselves  using complex deep neural networks. The system accepts large datasets of source codes from the internet and trains neural networks based on these datasets. It focusses at Domain Specific Languages, The neural net predicts the probability of occurrence of a higher level function in the code, based on these probabilities and with the help of various search techniques the  neural net produces code based in the DSL
    Although this project is not directly related to our idea it forms the basis of the concept of producing deployable code based on deep learning techniques and high computation. Multiple research publication cite this paper in the field of computer vision as it is considered as  a cornerstone. This project fueled the concept of generating code using code based on vision, imagery, structured data and even other source codes.




%_____________________________________________________________________________________________
